{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU,Dense\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./raw_stocks.csv')\n",
    "#单独取出股票的数据以及去除退市、st的数据\n",
    "data = data.loc[(data.loc[:,'status']==1)&(data.loc[:,'isST']==0),:]\n",
    "data.loc[:,['amount','volume']] = data.loc[:,['amount','volume']].fillna(0)\n",
    "data = data.fillna(1)\n",
    "data.loc[:,['open', 'high', 'low', 'close', 'preclose', 'volume','amount', 'pbMRQ', \\\n",
    "            'peTTM', 'turn', 'tradestatus', 'pctChg']]= \\\n",
    "                data.loc[:,['open', 'high', 'low',  'close', 'preclose', 'volume','amount',\\\n",
    "                             'pbMRQ', 'peTTM', 'turn', 'tradestatus', 'pctChg']].astype('float')\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "codes = data.loc[:,'code'].unique().tolist()\n",
    "min_date = data.date.min()\n",
    "max_date = data.date.max()\n",
    "date_series = data.date.unique()\n",
    "date_series = sorted(date_series)\n",
    "date_list = date_series*len(codes)\n",
    "\n",
    "ndate = len(date_series)\n",
    "code_ndate_list = []\n",
    "for i in codes:\n",
    "    temp_l = [i]*ndate\n",
    "    code_ndate_list.extend(temp_l)\n",
    "code_ndate_df = pd.DataFrame(zip(code_ndate_list,date_list))\n",
    "code_ndate_df.columns=['code','date']\n",
    "\n",
    "code_ndate_df = code_ndate_df.merge(data,how='left',on=['code','date'])\n",
    "#data = data.merge(date_df,how='left',left_on='date',right_on='date_dt')\n",
    "#data.drop(columns = ['date_dt','is_holiday'],inplace=True)\n",
    "#data.head(10)\n",
    "data = code_ndate_df\n",
    "del code_ndate_df\n",
    "\n",
    "data.sort_values(by=['code','date'],inplace=True)\n",
    "\n",
    "data.reset_index(drop=True,inplace=True)\n",
    "\n",
    "data.loc[:,'isonline'] = 1\n",
    "data.loc[data.close.isna(),'isonline']=0\n",
    "\n",
    "\"\"\"\n",
    "data.loc[:,['open', 'high', 'low', 'close', 'preclose', 'volume','amount', 'pbMRQ', 'peTTM', 'turn', 'tradestatus', 'pctChg','industry']]\\\n",
    "    = data.groupby(['code'])[['open', 'high', 'low', 'close', 'preclose', 'volume','amount', 'pbMRQ', \\\n",
    "            'peTTM', 'turn', 'tradestatus', 'pctChg','industry']].fillna(method='bfill')\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "data['dayofweek'] = data['date'].dt.dayofweek\n",
    "data.loc[:,'market']= data.code.str[0:2]\n",
    "#data_sh = data.loc[data.loc[:,'market']=='sh',:]\n",
    "data = data.loc[data.loc[:,'market']=='sz',:]\n",
    "data = data.drop(columns=['market'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nna = data.loc[data.loc[:,'close'].isna(),:].groupby('code',as_index=False)['date'].count()\n",
    "data = data.loc[~data.code.isin(nna.code),:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code           0\n",
       "date           0\n",
       "open           0\n",
       "high           0\n",
       "low            0\n",
       "close          0\n",
       "preclose       0\n",
       "volume         0\n",
       "amount         0\n",
       "pbMRQ          0\n",
       "peTTM          0\n",
       "turn           0\n",
       "tradestatus    0\n",
       "pctChg         0\n",
       "isST           0\n",
       "industry       0\n",
       "type           0\n",
       "status         0\n",
       "isonline       0\n",
       "year           0\n",
       "month          0\n",
       "day            0\n",
       "dayofweek      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_trend = data.groupby(['date'],as_index=False)[['volume','amount']].sum()\n",
    "gen_trend = gen_trend.sort_values(by=['date'],ascending=True).reset_index(drop=True)\n",
    "#增加行业一天以上的数据情况\n",
    "industry_trend = data.groupby(['date','industry'],as_index=False)[['volume','amount']].sum()\n",
    "industry_trend = industry_trend.sort_values(by=['industry','date'],ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_series = pd.date_range(start=min_date,end = max_date,freq='D')\n",
    "date_df =pd.DataFrame(date_series)\n",
    "date_df.columns=['date']\n",
    "#针对不同类型的holiday做了不同权重的分配\n",
    "from chinese_calendar import is_holiday,get_holiday_detail\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#lencoder = LabelEncoder()\n",
    "holidays = {\"New Year's Day\":1,\"Spring Festival\":7,\"Mid-autumn Festival\":3,\"Labour Day\":2,\"Tomb-sweeping Day\":4,\"Dragon Boat Festival\":3,\"National Day\":6}\n",
    "date_df.loc[:,'is_holiday'] = pd.to_datetime(date_df.loc[:,'date']).apply(lambda x:get_holiday_detail(x)[1])\n",
    "date_df.loc[:,'is_holiday'] = date_df.loc[:,'is_holiday'].apply(lambda x:holidays[x] if x in holidays else 0)\n",
    "date_df.loc[:,'is_holiday'].groupby(date_df.loc[:,'is_holiday']).count()\n",
    "date_df.loc[:,'last_holiday'] = date_df.loc[:,'is_holiday'].shift(1)\n",
    "date_df.loc[:,'next_holiday'] = date_df.loc[:,'is_holiday'].shift(-1)\n",
    "date_df.loc[:,'last2_holiday'] = date_df.loc[:,'is_holiday'].shift(2)\n",
    "date_df.loc[:,'next2_holiday'] = date_df.loc[:,'is_holiday'].shift(-2)\n",
    "date_df.loc[:,['last_holiday','next_holiday','last2_holiday','next2_holiday']]=date_df.loc[:,['last_holiday','next_holiday','last2_holiday','next2_holiday']].fillna(0)\n",
    "date_df.loc[:,['last_holiday','next_holiday','last2_holiday','next2_holiday']] = date_df.loc[:,['last_holiday','next_holiday','last2_holiday','next2_holiday']].astype('int')\n",
    "date_df.loc[:,'date_dt']= pd.to_datetime(date_df.loc[:,'date'])\n",
    "date_df.drop(columns=['date'],inplace=True)\n",
    "data.date = pd.to_datetime(data.date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(date_df,how='left',left_on='date',right_on='date_dt')\n",
    "\n",
    "data.drop(columns=['date_dt','is_holiday'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_trend.rename(columns ={'amount':'market_amt','volume':'market_vol'},inplace=True)\n",
    "industry_trend.rename(columns = {'amount':'indu_amount','volume':'indu_volume'},inplace=True)\n",
    "data = data.merge(gen_trend,how='left',on='date')\n",
    "data = data.merge(industry_trend,how='left',on=['date','industry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [3,5,12,20]  # 可以根据需要调整窗口大小\n",
    "rol_cols = []\n",
    "for window in windows:\n",
    "    rolling_mean = data.groupby(['code'],as_index=False)[['open','high','low','close','volume','amount']].rolling(window=window).mean()\n",
    "    rolling_mean.columns = ['code']+[f'{col}_roll_mean_{window}' for col in rolling_mean.columns if col not in ['code']]\n",
    "    rolling_mean.drop(columns=['code'],inplace=True)\n",
    "    rolling_std = data.groupby(['code'],as_index=False)[['open','high','low','close','volume','amount','pbMRQ','peTTM','turn']].rolling(window=window).std()\n",
    "    rolling_std.columns = ['code']+[f'{col}_roll_std_{window}' for col in rolling_std.columns if col not in ['code']]\n",
    "    rolling_std.drop(columns=['code'],inplace=True)\n",
    "    df = pd.concat([rolling_mean,rolling_std],axis=1)\n",
    "    rol_cols.append(df)\n",
    "lags = [5,12,20]  # 可以根据需要调整滞后阶数\n",
    "for lag in lags:\n",
    "    shift_value = data.groupby(['code'],as_index=False)[['open','high','low','close','volume','amount','pbMRQ','peTTM','turn']].shift(lag)\n",
    "    shift_value.columns = [f'{col}_roll_shift_{lag}' for col in shift_value.columns if col not in ['code']]\n",
    "    #shift_value.drop(columns=['code'],inplace=True)\n",
    "    rol_cols.append(shift_value)\n",
    "leads = [-3,-4,-5,-6,-7]\n",
    "\n",
    "y_cols=[]\n",
    "for lead in leads:\n",
    "    shift_value = data.groupby(['code'],as_index=False)[['close']].shift(lead)\n",
    "    shift_value.loc[:,'increase'] = shift_value.loc[:,'close']/data.loc[:,'close']-1\n",
    "    shift_value.loc[:,'if_inc'] = 0\n",
    "    shift_value.loc[shift_value.loc[:,'increase']>=0.1,'if_inc']=1\n",
    "    new_shift_value = pd.DataFrame(shift_value.loc[:,'if_inc'])\n",
    "    temp_cols = [f'{col}_increase_lead_{lead}' for col in new_shift_value.columns if col not in ['code']]\n",
    "    new_shift_value.columns = temp_cols\n",
    "    rol_cols.append(new_shift_value)\n",
    "    y_cols.extend(temp_cols)\n",
    "df = pd.concat(rol_cols,axis=1)\n",
    "\n",
    "data = pd.concat([data,df],axis=1)\n",
    "del df,date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_y_cols = data.loc[:,y_cols].astype('int')\n",
    "desc_y_cols.loc[:,'sum_col'] = desc_y_cols.sum(axis=1)\n",
    "desc_y_cols.loc[:,'if_inc'] = desc_y_cols.loc[:,'sum_col'].apply(lambda x:1 if x>0 else 0 )\n",
    "#desc_y_cols.loc[:,'if_inc'].groupby(desc_y_cols.loc[:,'if_inc']).count()\n",
    "data.loc[:,'if_inc'] = desc_y_cols.loc[:,'if_inc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "if_inc\n",
       "0    1579035\n",
       "1     203517\n",
       "Name: if_inc, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[:,'if_inc'].groupby(data.loc[:,'if_inc']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['volume_roll_std_5',\n",
       " 'close_roll_shift_20',\n",
       " 'close_roll_shift_12',\n",
       " 'turn_roll_std_20',\n",
       " 'turn_roll_std_3',\n",
       " 'isonline',\n",
       " 'peTTM',\n",
       " 'amount_roll_mean_5',\n",
       " 'peTTM_roll_std_3',\n",
       " 'next_holiday',\n",
       " 'high_roll_shift_12',\n",
       " 'open_roll_std_20',\n",
       " 'pbMRQ_roll_shift_12',\n",
       " 'preclose',\n",
       " 'close_roll_std_20',\n",
       " 'high',\n",
       " 'turn',\n",
       " 'high_roll_std_5',\n",
       " 'volume_roll_mean_12',\n",
       " 'amount_roll_shift_20',\n",
       " 'open_roll_shift_12',\n",
       " 'low_roll_shift_12',\n",
       " 'turn_roll_shift_12',\n",
       " 'volume_roll_shift_20',\n",
       " 'last2_holiday',\n",
       " 'open_roll_std_3',\n",
       " 'pbMRQ_roll_shift_5',\n",
       " 'open',\n",
       " 'open_roll_std_12',\n",
       " 'volume_roll_std_3',\n",
       " 'open_roll_mean_5',\n",
       " 'turn_roll_std_5',\n",
       " 'peTTM_roll_shift_12',\n",
       " 'pbMRQ',\n",
       " 'close_roll_std_12',\n",
       " 'high_roll_std_12',\n",
       " 'amount_roll_shift_5',\n",
       " 'close_roll_shift_5',\n",
       " 'open_roll_mean_20',\n",
       " 'amount_roll_mean_12',\n",
       " 'close_roll_mean_3',\n",
       " 'low_roll_std_3',\n",
       " 'amount',\n",
       " 'amount_roll_mean_3',\n",
       " 'close_roll_mean_20',\n",
       " 'volume_roll_mean_20',\n",
       " 'pbMRQ_roll_std_20',\n",
       " 'close_roll_std_3',\n",
       " 'peTTM_roll_std_5',\n",
       " 'indu_amount',\n",
       " 'high_roll_std_3',\n",
       " 'open_roll_std_5',\n",
       " 'pbMRQ_roll_std_12',\n",
       " 'open_roll_mean_3',\n",
       " 'high_roll_mean_20',\n",
       " 'high_roll_shift_5',\n",
       " 'high_roll_shift_20',\n",
       " 'peTTM_roll_shift_5',\n",
       " 'market_amt',\n",
       " 'volume',\n",
       " 'market_vol',\n",
       " 'pbMRQ_roll_shift_20',\n",
       " 'peTTM_roll_std_20',\n",
       " 'day',\n",
       " 'last_holiday',\n",
       " 'low_roll_std_20',\n",
       " 'volume_roll_shift_5',\n",
       " 'turn_roll_shift_20',\n",
       " 'low_roll_mean_5',\n",
       " 'low_roll_std_5',\n",
       " 'tradestatus',\n",
       " 'pbMRQ_roll_std_3',\n",
       " 'low_roll_mean_12',\n",
       " 'pctChg',\n",
       " 'high_roll_std_20',\n",
       " 'volume_roll_std_20',\n",
       " 'low_roll_shift_5',\n",
       " 'amount_roll_mean_20',\n",
       " 'amount_roll_std_20',\n",
       " 'amount_roll_shift_12',\n",
       " 'month',\n",
       " 'volume_roll_mean_3',\n",
       " 'amount_roll_std_12',\n",
       " 'open_roll_shift_5',\n",
       " 'low_roll_shift_20',\n",
       " 'close',\n",
       " 'low_roll_mean_3',\n",
       " 'volume_roll_std_12',\n",
       " 'low_roll_std_12',\n",
       " 'high_roll_mean_5',\n",
       " 'amount_roll_std_3',\n",
       " 'open_roll_shift_20',\n",
       " 'next2_holiday',\n",
       " 'low_roll_mean_20',\n",
       " 'volume_roll_shift_12',\n",
       " 'turn_roll_std_12',\n",
       " 'peTTM_roll_std_12',\n",
       " 'high_roll_mean_12',\n",
       " 'amount_roll_std_5',\n",
       " 'turn_roll_shift_5',\n",
       " 'indu_volume',\n",
       " 'dayofweek',\n",
       " 'close_roll_mean_12',\n",
       " 'open_roll_mean_12',\n",
       " 'peTTM_roll_shift_20',\n",
       " 'high_roll_mean_3',\n",
       " 'year',\n",
       " 'close_roll_mean_5',\n",
       " 'close_roll_std_5',\n",
       " 'volume_roll_mean_5',\n",
       " 'low',\n",
       " 'pbMRQ_roll_std_5']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_cols = data.columns.tolist()\n",
    "ts_cols = list(set(ts_cols)-set(['date','code','isST','type','status','industry','if_inc'])-set(y_cols))\n",
    "ts_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_encoder = LabelEncoder()\n",
    "ind_encoder = LabelEncoder()\n",
    "data.loc[:,'code'] = code_encoder.fit_transform(data.loc[:,'code'])\n",
    "data.loc[:,'industry'] = ind_encoder.fit_transform(data.loc[:,'industry'].astype('str'))\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#data.loc[:,ts_cols] = scaler.fit_transform(data.loc[:,ts_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = ts_cols+['code','industry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['amount_roll_shift_20'],inplace=True,how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = data.isnull().sum()\n",
    "a[a>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = 132\n",
    "steps = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "def time_warp_multivariate(time_series,cat_n=2,distortion_factor=0.1):\n",
    "    \"\"\"\n",
    "    对多维时间序列应用时间扭曲。\n",
    "    \n",
    "    参数:\n",
    "    - time_series: 多维时间序列数据 (numpy array)\n",
    "    - distortion_factor: 扭曲强度因子\n",
    "    \n",
    "    返回:\n",
    "    - warped_series: 扭曲后的多维时间序列数据\n",
    "    \"\"\"\n",
    "    n_samples, n_timesteps, n_features = time_series.shape\n",
    "    warped_series = np.empty_like(time_series)\n",
    "    num_features = n_features-cat_n\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_features):\n",
    "            if j < num_features:\n",
    "                original_series = time_series[i, :, j]\n",
    "                # 扭曲原始时间轴\n",
    "                original_time = np.arange(n_timesteps)\n",
    "                distortion = distortion_factor * np.random.randn(n_timesteps)\n",
    "                warped_time = original_time + distortion\n",
    "                warped_time = np.clip(warped_time, 0, n_timesteps - 1)\n",
    "                \n",
    "                # 插值以生成扭曲后的时间序列\n",
    "                interpolation = interp1d(warped_time, original_series, kind='linear', fill_value='extrapolate')\n",
    "                warped_series[i, :, j] = interpolation(original_time)\n",
    "            else:\n",
    "                warped_series[i, :, j] = original_time\n",
    "    return warped_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2056/2056 [01:36<00:00, 21.32it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def create_date_window(df, date_col, code_col, x_cols, label_col, train_length, steps,ratio=0.6,keep=-3):\n",
    "    # 获取唯一的代码列表\n",
    "    codes = df[code_col].unique()\n",
    "    \n",
    "    X_train,X_eval,X_test, y_train,y_eval,y_test,X_pred = [], [],[],[],[],[],[]\n",
    "    \n",
    "    for code in tqdm(codes):\n",
    "        # 筛选出特定代码的数据并按日期排序\n",
    "        df_code = df[df[code_col] == code].sort_values(by=date_col)\n",
    "        \n",
    "        dates = df_code[date_col].values\n",
    "        labels = df_code[label_col].values\n",
    "        data = df_code[x_cols].astype('float32').values\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        data = scaler.fit_transform(data)\n",
    "\n",
    "        n = len(dates)\n",
    "        m = n%steps\n",
    "        s = n//(train_length+steps)\n",
    "           \n",
    "        X,y = [],[]\n",
    "        for i in range(m, n - train_length, steps):\n",
    "            end_index = i + train_length\n",
    "            \n",
    "            temp_ar = data[i:end_index]\n",
    "            temp_labels = labels[i:end_index]\n",
    "            if temp_ar.shape[0] == train_length:\n",
    "                X.append(temp_ar)\n",
    "                y.append(temp_labels[-1])\n",
    "\n",
    "        if len(X) != 0:\n",
    "            #ratio=0.6\n",
    "            train_size = int(ratio*(len(X)-1))\n",
    "            eval_size = int((1-ratio)/2*(len(X)-1))\n",
    "\n",
    "            X_train.extend(X[0:train_size])\n",
    "            y_train.extend(y[0:train_size])\n",
    "            #X_eval.extend(X[train_size:(train_size+eval_size)])\n",
    "            #y_eval.extend(y[train_size:(train_size+eval_size)])\n",
    "            #X_test.extend(X[(train_size+eval_size):-1])\n",
    "            #y_test.extend(y[(train_size+eval_size):-1])\n",
    "            X_eval.extend(X[train_size:keep])\n",
    "            y_eval.extend(y[train_size:keep])\n",
    "            X_test.append(X[keep])\n",
    "            y_test.append(y[keep])\n",
    "            X_pred.append([X[-1]])\n",
    "\n",
    "    X_train= np.array(X_train)\n",
    "    X_eval = np.array(X_eval)\n",
    "    y_train = np.array(y_train,dtype=np.float32)\n",
    "    y_eval=np.array(y_eval,dtype=np.float32)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test,dtype=np.float32)\n",
    "    X_pred = np.array(X_pred)\n",
    "\n",
    "    #X_train = time_warp_multivariate(X_train, distortion_factor=0.1)\n",
    "    #X_eval = time_warp_multivariate(X_eval, distortion_factor=0.1)\n",
    "    #X_pred = time_warp_multivariate(X_pred, distortion_factor=0.1)\n",
    "\n",
    "    \n",
    "    return X_train,X_eval,X_test, y_train,y_eval,y_test,X_pred\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "del data\n",
    "date_col = 'date'\n",
    "code_col = 'code'\n",
    "label_col = 'if_inc'\n",
    "\n",
    "train_length = 132\n",
    "steps = 7\n",
    "\n",
    "\n",
    "\n",
    "X_train,X_eval,X_test, y_train,y_eval,y_test,X_pred = create_date_window(df, date_col, code_col, x_cols, label_col, train_length, steps,0.6,-2)\n",
    "\n",
    "del df\n",
    "#print(\"X shape:\", X_train.shape)  # 输出应为 (num_samples, train_length, num_features)\n",
    "#print(\"y shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14129,)\n"
     ]
    }
   ],
   "source": [
    "#diy under sample\n",
    "indices = np.where(y_train==0)[0]\n",
    "ind_1 = np.where(y_train==1)[0]\n",
    "X_train_cls_0 = X_train[indices]\n",
    "X_train_cls_1 = X_train[ind_1]\n",
    "\n",
    "ratio = y_train[y_train==1].shape[0]/y_train[y_train==0].shape[0]\n",
    "sample_size = int(ratio*len(X_train_cls_0))\n",
    "sample_indices = np.random.choice(len(X_train_cls_0),sample_size,replace=False)\n",
    "sampled_X_train_cls_0 = X_train_cls_0[sample_indices]\n",
    "sampled_y_train_cls_0 = y_train[sample_indices]\n",
    "print(sampled_y_train_cls_0.shape)\n",
    "\n",
    "resampled_train_X = np.concatenate([X_train_cls_1,sampled_X_train_cls_0],axis=0)\n",
    "resampled_train_y = np.concatenate([y_train[ind_1],sampled_y_train_cls_0],axis=0)\n",
    "\n",
    "order = np.arange(len(resampled_train_y))\n",
    "np.random.shuffle(order)\n",
    "\n",
    "resampled_train_X = resampled_train_X[order]\n",
    "resampled_train_y = resampled_train_y[order]\n",
    "\n",
    "del X_train_cls_0,X_train_cls_1,sampled_X_train_cls_0,sampled_y_train_cls_0,X_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smote "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28258, 132, 114)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "442/442 [==============================] - 147s 324ms/step - loss: 4.0586 - recall_9: 0.7504 - precision_9: 0.5929 - prc: 0.6156 - val_loss: 0.8242 - val_recall_9: 0.9621 - val_precision_9: 0.0942 - val_prc: 0.1205\n",
      "Epoch 2/50\n",
      "442/442 [==============================] - 106s 241ms/step - loss: 0.6977 - recall_9: 0.7859 - precision_9: 0.5931 - prc: 0.6306 - val_loss: 0.9306 - val_recall_9: 0.9868 - val_precision_9: 0.0948 - val_prc: 0.1307\n",
      "Epoch 3/50\n",
      "442/442 [==============================] - 107s 242ms/step - loss: 0.6940 - recall_9: 0.7732 - precision_9: 0.5936 - prc: 0.6255 - val_loss: 0.8513 - val_recall_9: 0.9581 - val_precision_9: 0.0961 - val_prc: 0.1026\n",
      "Epoch 4/50\n",
      "442/442 [==============================] - 104s 235ms/step - loss: 0.6930 - recall_9: 0.7601 - precision_9: 0.5979 - prc: 0.6321 - val_loss: 0.7202 - val_recall_9: 0.4850 - val_precision_9: 0.1185 - val_prc: 0.1230\n",
      "Epoch 5/50\n",
      "442/442 [==============================] - 102s 231ms/step - loss: 0.6899 - recall_9: 0.7596 - precision_9: 0.5957 - prc: 0.6287 - val_loss: 0.7933 - val_recall_9: 0.8433 - val_precision_9: 0.1043 - val_prc: 0.1203\n",
      "Epoch 6/50\n",
      "442/442 [==============================] - 104s 236ms/step - loss: 0.6871 - recall_9: 0.7328 - precision_9: 0.6043 - prc: 0.6370 - val_loss: 0.8145 - val_recall_9: 0.8719 - val_precision_9: 0.1013 - val_prc: 0.1194\n",
      "Epoch 7/50\n",
      "442/442 [==============================] - 100s 227ms/step - loss: 0.6832 - recall_9: 0.7277 - precision_9: 0.6098 - prc: 0.6420 - val_loss: 0.7578 - val_recall_9: 0.6289 - val_precision_9: 0.1106 - val_prc: 0.1234\n",
      "Epoch 8/50\n",
      "442/442 [==============================] - 101s 229ms/step - loss: 0.6807 - recall_9: 0.7307 - precision_9: 0.6109 - prc: 0.6517 - val_loss: 0.6723 - val_recall_9: 0.4943 - val_precision_9: 0.1231 - val_prc: 0.1247\n",
      "Epoch 9/50\n",
      "442/442 [==============================] - 101s 229ms/step - loss: 0.6756 - recall_9: 0.7144 - precision_9: 0.6250 - prc: 0.6658 - val_loss: 0.6179 - val_recall_9: 0.1989 - val_precision_9: 0.1103 - val_prc: 0.1092\n",
      "Epoch 10/50\n",
      "442/442 [==============================] - 100s 226ms/step - loss: 0.6743 - recall_9: 0.7098 - precision_9: 0.6257 - prc: 0.6662 - val_loss: 0.7499 - val_recall_9: 0.6434 - val_precision_9: 0.1019 - val_prc: 0.1019\n",
      "Epoch 11/50\n",
      "442/442 [==============================] - 100s 227ms/step - loss: 0.6710 - recall_9: 0.7202 - precision_9: 0.6291 - prc: 0.6729 - val_loss: 0.7600 - val_recall_9: 0.6008 - val_precision_9: 0.1037 - val_prc: 0.1077\n",
      "Epoch 12/50\n",
      "442/442 [==============================] - 104s 236ms/step - loss: 0.6680 - recall_9: 0.7307 - precision_9: 0.6335 - prc: 0.6806 - val_loss: 0.6180 - val_recall_9: 0.2451 - val_precision_9: 0.1048 - val_prc: 0.1014\n",
      "Epoch 13/50\n",
      "442/442 [==============================] - 702s 2s/step - loss: 0.6656 - recall_9: 0.7167 - precision_9: 0.6377 - prc: 0.6832 - val_loss: 0.6856 - val_recall_9: 0.4063 - val_precision_9: 0.1152 - val_prc: 0.1147\n",
      "Epoch 14/50\n",
      "442/442 [==============================] - 226s 512ms/step - loss: 0.6646 - recall_9: 0.7195 - precision_9: 0.6389 - prc: 0.6872 - val_loss: 0.6931 - val_recall_9: 0.4338 - val_precision_9: 0.1121 - val_prc: 0.1160\n",
      "Epoch 15/50\n",
      "442/442 [==============================] - 109s 247ms/step - loss: 0.6636 - recall_9: 0.7210 - precision_9: 0.6399 - prc: 0.6900 - val_loss: 0.7204 - val_recall_9: 0.5037 - val_precision_9: 0.1037 - val_prc: 0.1127\n",
      "Epoch 16/50\n",
      "442/442 [==============================] - 108s 244ms/step - loss: 0.6616 - recall_9: 0.7124 - precision_9: 0.6431 - prc: 0.6922 - val_loss: 0.7863 - val_recall_9: 0.5812 - val_precision_9: 0.0878 - val_prc: 0.1048\n",
      "Epoch 17/50\n",
      "442/442 [==============================] - 107s 243ms/step - loss: 0.6615 - recall_9: 0.7114 - precision_9: 0.6434 - prc: 0.6908 - val_loss: 0.5323 - val_recall_9: 0.0984 - val_precision_9: 0.0967 - val_prc: 0.0955\n",
      "Epoch 18/50\n",
      "442/442 [==============================] - 106s 241ms/step - loss: 0.6623 - recall_9: 0.7156 - precision_9: 0.6418 - prc: 0.6878 - val_loss: 0.7616 - val_recall_9: 0.5863 - val_precision_9: 0.1034 - val_prc: 0.1169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1364f2130>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GRU, Dense, Dropout,BatchNormalization,Bidirectional\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.metrics import Recall,Precision,F1Score,AUC\n",
    "#from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "from tensorflow.keras.optimizers import legacy\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "#from tensorflow.keras.optimizers.legacy import Adam\n",
    "#class_weights = {0: 1.0, 1: y_train[y_train==0].shape[0]/y_train[y_train==1].shape[0]}\n",
    "#class_weights = {0: 1.0, 1: resampled_train_y[resampled_train_y==0].shape[0]/resampled_train_y[resampled_train_y==1].shape[0]}\n",
    "# 定义模型\n",
    "model = Sequential()\n",
    "\n",
    "# 添加 1D 卷积层：因为卷基层有更好的特征提取能力，kernel_size为3 会生成一个大小为3的时间窗提取特征，因此kernel_size越小，会提取更多更细微的特征。\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu',kernel_regularizer=l2(0.03), input_shape=(train_length, len(x_cols)),kernel_initializer=initializers.HeNormal()))\n",
    "#批量归一化：作用1.对每一层的输入进行标准化 2.减少内部协变量偏移 3.提供正则化效果\n",
    "model.add(BatchNormalization())  \n",
    "#池化：提取重要特征 例：[1,2,3,4,5,6,7,8] 经过pool_size=2的池化就会提取出【2，4，6，8】\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# 添加更多卷积层：网络越深，可以增加kernel_size\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu',kernel_regularizer=l2(0.03)))\n",
    "model.add(BatchNormalization())  \n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# 添加 GRU 层\n",
    "model.add(Bidirectional(GRU(128, return_sequences=True,kernel_initializer=initializers.HeNormal(),recurrent_dropout=0.3)))\n",
    "model.add(BatchNormalization())  \n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# 添加更多的 GRU 层（可选）\n",
    "model.add(Bidirectional(GRU(64,kernel_initializer=initializers.HeNormal(),recurrent_dropout=0.3)))\n",
    "model.add(BatchNormalization())  \n",
    "model.add(Dropout(0.4))\n",
    "# 添加输出层\n",
    "model.add(Dense(128, activation='relu', kernel_initializer=initializers.HeNormal(),kernel_regularizer=l2(0.03))) \n",
    "model.add(Dense(64, activation='relu', kernel_initializer=initializers.HeNormal(),kernel_regularizer=l2(0.03))) \n",
    "model.add(Dense(1, activation='sigmoid',kernel_initializer=initializers.HeNormal()))  # 对于回归任务可以使用 'linear'\n",
    "\n",
    "optimizer = legacy.Adam(learning_rate=0.001,clipnorm=1.0)\n",
    "#optimizer = RMSprop(learning_rate=0.001)\n",
    "# 编译模型\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[Recall(),Precision(),AUC(name='prc', curve='PR')])\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_precision_9', \n",
    "    mode = 'max',          # 监控的指标，这里选择验证集上的损失\n",
    "    patience=10,                # 在指标停止改进后的容忍轮次\n",
    "    restore_best_weights=True     # 是否恢复到最佳的权重\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "#model.fit(X_train,y_train, epochs=50, batch_size=64, validation_data=(X_eval, y_eval), callbacks=[early_stopping],class_weight=class_weights)\n",
    "model.fit(resampled_train_X,resampled_train_y, epochs=50, batch_size=64, validation_data=(X_eval, y_eval), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('conv_gru_blance_prc.h5')\n",
    "#model.save('conv_gru_blance_prc.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#GRU 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 1s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true,y_pred):\n",
    "    #\n",
    "    data = pd.DataFrame({'y_true':y_true,'y_pred':y_pred})\n",
    "    data.loc[:,'rnk'] = data.loc[:,'y_pred'].rank(ascending=False)\n",
    "    nsamples = len(data)\n",
    "    data.loc[:,'pert'] = data.loc[:,'rnk']/nsamples\n",
    "    data.loc[:,'cat']= pd.cut(data.loc[:,'y_pred'],bins=10)\n",
    "    new_df = data.groupby(['cat'])['y_true'].agg(['count','sum'])\n",
    "    new_df.loc[:,'id']= [i for i in range(len(new_df))]\n",
    "    new_df.sort_values(by=['id'],ascending=False,inplace=True)\n",
    "    new_df.reset_index(inplace=True)\n",
    "    new_df.loc[:,'accsum']= new_df.loc[:,'sum'].cumsum()\n",
    "    good_cnt = new_df.loc[:,'sum'].sum()\n",
    "    new_df.loc[:,'recall'] = new_df.loc[:,'accsum']/good_cnt\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        prob  act\n",
      "0   0.722259  0.0\n",
      "1   0.717964  0.0\n",
      "2   0.715934  1.0\n",
      "3   0.712860  0.0\n",
      "4   0.712605  0.0\n",
      "5   0.712443  0.0\n",
      "6   0.710410  0.0\n",
      "7   0.710308  0.0\n",
      "8   0.709085  0.0\n",
      "9   0.708576  1.0\n",
      "10  0.706418  0.0\n",
      "top 10 acc:  2.0\n",
      "              cat  count   sum  id  accsum    recall\n",
      "0  (0.687, 0.722]     42   7.0   9     7.0  0.050000\n",
      "1  (0.651, 0.687]     70   5.0   8    12.0  0.085714\n",
      "2  (0.615, 0.651]    181  15.0   7    27.0  0.192857\n",
      "3   (0.58, 0.615]    451  29.0   6    56.0  0.400000\n",
      "4   (0.544, 0.58]    428  26.0   5    82.0  0.585714\n",
      "5  (0.509, 0.544]    338  27.0   4   109.0  0.778571\n",
      "6  (0.473, 0.509]    226  17.0   3   126.0  0.900000\n",
      "7  (0.437, 0.473]    147  14.0   2   140.0  1.000000\n",
      "8  (0.402, 0.437]     91   0.0   1   140.0  1.000000\n",
      "9  (0.366, 0.402]     82   0.0   0   140.0  1.000000\n"
     ]
    }
   ],
   "source": [
    "result = pd.concat([pd.Series(y_pred.reshape((-1,))),pd.Series(y_test.reshape((-1,)))],axis=1,ignore_index=True)\n",
    "result.columns = ['prob','act']\n",
    "result = result.sort_values(by=['prob'],ascending=False).reset_index(drop=True)\n",
    "print(result.loc[0:10,:])\n",
    "print(\"top 10 acc: \",result.loc[0:10,'act'].sum())\n",
    "cls_eval = evaluate_model(pd.Series(y_test.reshape((-1,))),pd.Series(y_pred.reshape((-1,))))\n",
    "#print(f\"dateis:{date_test[i]}\")\n",
    "print(cls_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22064424],\n",
       "       [0.22064424],\n",
       "       [0.22064424],\n",
       "       ...,\n",
       "       [0.22064424],\n",
       "       [0.22064424],\n",
       "       [0.22064424]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Focal Loss for classification.\n",
    "    :param gamma: Focusing parameter (default=2.0).\n",
    "    :param alpha: Balance parameter (default=0.25).\n",
    "    :return: A loss function.\n",
    "    \"\"\"\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        # Ensure alpha is of float type\n",
    "        #alpha = tf.cast(alpha, tf.float32)\n",
    "        #gamma = tf.cast(gamma, tf.float32)\n",
    "        \n",
    "        # 1. Define epsilon for numerical stability\n",
    "        epsilon = K.epsilon()\n",
    "        \n",
    "        # 2. Clip predictions to avoid log(0) issues\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        # 3. Calculate alpha_t for each sample\n",
    "        alpha_t = y_true * alpha + (K.ones_like(y_true) - y_true) * (1 - alpha)\n",
    "        \n",
    "        # 4. Calculate p_t for each sample\n",
    "        p_t = y_true * y_pred + (K.ones_like(y_true) - y_true) * (1 - y_pred)\n",
    "        \n",
    "        # 5. Calculate focal loss\n",
    "        fl = - alpha_t * K.pow((1 - p_t), gamma) * K.log(p_t)\n",
    "        \n",
    "        # 6. Return mean of focal loss\n",
    "        return K.mean(fl)\n",
    "    \n",
    "    return focal_loss_fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxyang/miniconda3/envs/stockpy38/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('cov_gru_128_loss_bidirection_features_blance.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the Positional Encoding layer to add time-related information to the input\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a positional encoding matrix (max_len, d_model) with sine and cosine functions\n",
    "        self.pos_encoding = self.positional_encoding(max_len, d_model)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def positional_encoding(self, max_len, d_model):\n",
    "        angle_rads = self.get_angles(tf.range(max_len)[:, tf.newaxis],\n",
    "                                     tf.range(d_model)[tf.newaxis, :],\n",
    "                                     d_model)\n",
    "        # Apply sin to even indices in the array; cos to odd indices\n",
    "        angle_rads[:, 0::2] = tf.math.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = angle_rads[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Add positional encoding to the input tensor\n",
    "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n",
    "\n",
    "# Multi-head attention layer\n",
    "def create_attention_head(d_model, num_heads):\n",
    "    return tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "\n",
    "# Transformer Encoder Layer\n",
    "def transformer_encoder_layer(d_model, num_heads, ff_dim, dropout_rate=0.1):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model))\n",
    "    \n",
    "    # Multi-head self-attention\n",
    "    attention_output = create_attention_head(d_model, num_heads)(inputs, inputs)\n",
    "    attention_output = tf.keras.layers.Dropout(dropout_rate)(attention_output)\n",
    "    attention_output = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_output + inputs)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ff_output = tf.keras.layers.Dense(ff_dim, activation='relu')(attention_output)\n",
    "    ff_output = tf.keras.layers.Dense(d_model)(ff_output)\n",
    "    ff_output = tf.keras.layers.Dropout(dropout_rate)(ff_output)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(ff_output + attention_output)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Full Transformer model for time series classification\n",
    "def create_transformer_model(num_features, d_model, num_heads, ff_dim, num_layers, num_classes):\n",
    "    inputs = tf.keras.Input(shape=(None, num_features))\n",
    "    \n",
    "    # Linear layer to map input features to d_model dimensions\n",
    "    x = tf.keras.layers.Dense(d_model)(inputs)\n",
    "    \n",
    "    # Positional encoding layer\n",
    "    x = PositionalEncoding(d_model)(x)\n",
    "    \n",
    "    # Stacking transformer encoder layers\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder_layer(d_model, num_heads, ff_dim)(x)\n",
    "    \n",
    "    # Use the first time step (or another aggregation method) for classification\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Output layer for classification\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Example of training the transformer model for classification\n",
    "\n",
    "# Assuming you have input data X_train of shape (batch_size, sequence_length, num_features)\n",
    "X_train = tf.random.normal((32, 132, 114))  # 32 samples, 132 time steps, 114 features\n",
    "y_train = tf.random.uniform((32,), maxval=2, dtype=tf.int32)  # Binary classification\n",
    "\n",
    "# Create the model\n",
    "model = create_transformer_model(num_features=114, d_model=64, num_heads=8, ff_dim=128, num_layers=2, num_classes=2)\n",
    "\n",
    "# Compile the model with loss function, optimizer, and evaluation metrics\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stockpy38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
